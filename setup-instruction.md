I would like to create a Python module that is able to give a plain sentence explanation to any kind of model input. My specific focus of this project is to make sure this module is able to output plain sentence to the text classification model. It should be able to explain the accuracy scores, recall and related probabilities, and some top words that appears more often in each class(categorization) of words. 

Here are some more details on project information. 

----

Abstract: Scholars with a less technical background are experiencing difficulties in understanding the black-box decisions made inside machine learning models and how they affect interpretation as the models become more complex. This project aims to create a Python library extension that enables Japanese data scientists to easily interpret model output in plain Japanese sentences, alongside visualizations and table outputs. The final deliverable would include a working Python library that outputs explanatory sentences with placeholders as an MVP, with an additional feature that allows the LLM to provide more personalized reports based on each input model's output. 

Humanities scholars want to use AI to analyze Japanese texts, but AI decisions feel like a "black box"—you can see the conclusion, but not why. This creates three major barriers:

Language incompatibility: AI explanation tools are built for English, which uses spaces between words. Japanese doesn't have spaces, so these tools break when processing Japanese text.

Knowledge gaps: Humanities scholars understand literature but lack technical backgrounds, while data scientists lack contextual knowledge for interpretation.

Trust issues: Without understanding how models work, researchers can't confidently use them for serious academic work.

I extended an existing Python library called LIME (Local Interpretable Model-Agnostic Explanations) to work with Japanese. LIME explains machine learning predictions, but it couldn't handle Japanese text. My project adds:

1. Japanese Text Processing - I integrated SudachiPy, a Japanese tokenizer that automatically breaks sentences into words when users specify Japanese language—no complicated setup needed.

2. Plain Language Explanations - Instead of just numbers and graphs, LIME.JP generates clear Japanese sentences. Example: "The word 'earthquake' greatly increased the probability of fake news classification (weight: +0.15)."

LIME.JP follows four steps: (1) breaks Japanese text into words using SudachiPy, (2) creates text variations by randomly removing words, (3) analyzes which words most affected the AI's predictions, and (4) translates findings into natural Japanese sentences that categorize each word's influence as "greatly," "moderately," or "slightly" affecting the result.
LIME.JP bridges a critical gap for Japanese NLP researchers. Currently, they must use expensive Windows-only tools like KH Coder ($200-300) or struggle adapting English tools. LIME.JP provides a free, accessible alternative that works with any machine learning model in the Python environment most data scientists already use.

In my demonstration analyzing fake news detection, LIME.JP revealed which words influenced the model's decisions—including some irrelevant words, helping researchers understand model limitations and decide if it's appropriate for their needs.
Next semester, I'll connect LIME.JP with large language models like ChatGPT to provide personalized, context-aware interpretations—explaining not just which words mattered, but why they matter in the specific dataset context.

Problems in Natural Language Processing Models
Natural Language Processing (NLP) algorithms have been providing numerous insights into the arts and humanities field over the past decades, but there is significantly less effort on how to bridge model results into literary interpretations. Although AI and machine learning methods have helped analyze literary datasets, arts and humanities scholars with less technical backgrounds often view those technologies as a “black box.” (Hatzel et al., 2023) This perception usually discourages scholars with less technical backgrounds from using new technologies, even if they recognize that these methods can be helpful. Generative AIs can provide generic suggestions on those “why” questions, but they often overcomplicate or even give false information. Those features are detrimental to scholars writing a research paper, as they need to be able to explain why a certain model works and why they can trust their output. 
	
	This problem is significant amog the users of NLP models because NLP is not only used by data science experts but also by the arts and humanities scholars who might have the historical resource they would like to analyze with computers. Data scientists might be able to create a sophisticated model, but they might not necessarily have the knowledge base over the implication and context of data. On the other hand, humanities scholars might not have perfect understanding about the model. Oxford Press survey study in 2024 shows that humanities scholars do not actively use AI tools compared to scholars in other field such as social sciences and STEM, showing that they typically have less incentive to learn the model structure itself. However, they have huge accumulated knowledge on how they can interpret the model results back into the context of literary data, which could bring meaningful result to the analysis. This gap between model output and contextual interpretation is one of the difficulties of being effective in using NLP in the humanities context, but also a potential to explore more interesting patterns in text data across the world.  
Problems in Handling Japanese Text Data
Japanese, as a language, is semantically different from many other languages in the world and therefore requires additional data processing to draw meaningful insights. The first main problem is that the model's approach to detecting separate words from a sentence differs significantly from English, the language with the most developed NLP models to date. In English, the models detect words by simply looking for the spaces in between the sentence. However, in Japanese, there are no spaces in between words. Therefore, the logic used for English would not apply to Japanese. The example below is the code I used for the past class to show this phenomenon. We can see that the string “なんやかんやありましたが4年目は台湾とインドに行くことになりました” (which translates into “I had different issues, but I can finally go to Taiwan and India during my fourth year”) is clearly not separated into smaller words. 


Figure 1: Code snippet from CS156 pipeline assignment that breaks down the sentence into separate words before pre-processing the Japanese text data to be usable in English-based models. 


Another problem is that Japanese is highly context-dependent, meaning that there are often some indirect components when drawing implications. For example, Japanese sentences in daily life sometimes do not contain a subject such as “I” or “he/she”. Although people can still communicate with those fragmental sentences, computer models often struggle with identifying which word connects which other word(s). Thus, they sometimes confuse the word-to-word connections and give misleading conclusions from those analyses. 



Given the challenges of interpretability and analyzing Japanese text data, I identified the need for a tool that can explain model output in Japanese, thereby reducing the black-box nature of NLP models for data scientists. This problem motivated me to think if there are any existing tools that aid scholars and data scientists to: 


Understand what the model did
Determine the trustworthiness of the model
Interpret them back into plain, written Japanese language, along with various model scores and plots


	In the next section, I will explain my process for searching existing tools and evaluating whether they meet the above criteria. 
Existing Implementations

The first visualization tool for natural language processing in Japanese which I found was KH Coder, a text-mining tool made by a Japanese developer that generates various types of visualizations of text analysis results. It is highly trusted by the Japanese academic community, with more than 8,000 scholarly papers using KH Coder having been published. However, KH Coder is only available on a Windows environment, and the source code is only accessible once you purchase the entire package, which costs approximately $200-300 USD in total. Given those points, KH Coders would be a nice choice if the researcher has good amount of budget for purchase, but might not be helpful for those who are tight on budget, such as students and early-career professionals. 
sensemakr performs a sensitivity analysis from causal inference models and gives a plain English explanation about the model output. This functionality is similar to what I aim to achieve with the NLP models in both Japanese and English. Since this entire module is written in R instead of Python, we would need to modify how I adapt this function into my own module. 
Looking into English resources, I found several modules working as an Explainable AI (XAI) where its goal is to provide more natural language explanation of the machine learning model output. The popular modules were LIME and SHAP, which mainly produces plots and visualizations for the text processing models. I also found Scikit-LLM, which performs classification tasks with OpenAI LLMs using the same interface as scikit-learn, a machine learning library popular among beginner to intermediate learners. However, the OpenAI API requires credits to make API calls, which would incur an additional cost on both the user and developer sides. Their structure might be useful for the future steps of this project, as described in a later section about connecting my module with LLMs; however, this would not be a single solution to the problem described above. 
Given those research, I decided to use LIME as a base of my new tool since it is fully written in Python and is widely accepted among the NLP community. In the following section, we will dive deeper into what is LIME and how it would be useful for our case. 
What is LIME?
Overview
LIME is an abbreviation for Locally Interpretable Model-Agnostic Explanations, and it is a Python module that provides a human-interpretable visualization of the machine learning model. The goal of LIME is to explain the predictions of any classifier in a faithful manner by approximating the complex "black-box" model locally with a simpler, interpretable one. Mathematically, LIME explains a prediction f()for a specific document  by examining the model's behavior in the immediate vicinity of that document.
The process begins by embedding the original document  into a vector representation, ()RD , typically utilizing a TF-IDF (Term Frequency-Inverse Document Frequency) transformer. In this schema, the value for a word wj in a document d is calculated as TF(d, wj)IDF(wj) , where the inverse document frequency is defined as IDF(wj)=logN+1Nj+1+1. Once the document is vectorized, LIME generates a set of n perturbed samples, denoted as x1, ... , xn, by randomly deleting words from the original document. The complex model f is then queried to obtain predictions for these perturbed samples, such that yi=f((xi)). Finally, a weighted linear surrogate model (such as Ridge regression) is trained to approximate f. This surrogate minimizes the loss between the black-box predictions yi and its own predictions, weighted heavily by how close the perturbed sample is to the original input. The resulting coefficients  of this linear model serve as interpretable indicators, revealing the positive or negative influence of specific words on the final prediction.
Strengths of LIME

The primary strength of LIME lies in its model-agnostic nature; it functions effectively with any machine learning model f, regardless of architecture, whether it be a neural network, random forest, or Support Vector Machine. Because LIME only requires the ability to query the model for predictions, users do not need access to the internal weights or structure of f to derive insights. Furthermore, LIME provides highly intuitive explanations by assigning interpretability weights to individual words. This allows human reviewers to easily verify model behavior and identify "spurious correlations," where a model might be relying on irrelevant or biased features rather than true signals. This makes it particularly valuable for Natural Language Processing (NLP) tasks, where the input features are words that carry inherent semantic meaning, unlike abstract pixel values in image classification. In terms of practical use, being model-agnostic opens up the product to a wider audience interested in different models, allowing all of them to benefit from the explainers. 
Limitations of LIME

One of the main limitations of LIME is its limitations in language processing and stability. A significant practical hurdle is that LIME's default text splitter, which relies on spacing, is unusable for languages like Japanese that do not use whitespace delimiters, as explained in the previous section. This makes it impossible to obtain meaningful word-level explanations without additional processing. For instance, Tachibana et al. (2021)  highlights this in a text mining analysis of recipe review data; the researchers were required to implement additional pre-processing steps, such as specialized tokenization, before they could successfully use LIME to obtain useful results. Beyond language issues, LIME lacks strong theoretical guarantees; even for simple models, there is no absolute assurance that the explanations are perfectly faithful. The method can also be unstable due to the random sampling process used to generate perturbations. If the sample size is not sufficiently large, different runs on the same input can yield different explanations. 
Implementation
Tokenizer

I integrated an internal tokenizer into LimeTextExplainer class to enable Japanese text to be directly passed into this module.  I selected SudachiPy for this implementation because it distributes an accompanying dictionary, sudachipy-dictionary, that is straightforward to install within a Python environment. This design choice avoids the configuration complexity associated with MeCab, another popular Japanese tokenizer, which often requires users to manually download and configure external dictionaries. SudachiPy is also known for efficient memory usage and high-speed processing, which is critical for NLP with large dataset. 
Plain Language Explanation
To bridge the gap between numerical output and user understanding, I developed narrative helper functions that translate LIME explanations into natural language. I implemented generate_sentence_for_feature_jp and summarize_lime_explanation_jp that is able to give plain sentence explanations about the model output while adopting language-appropriate phrasing and structure.
I implemented generate_sentence_for_feature_jp to provide a granular explanation of individual terms. This function takes a token, its LIME weight, and the class name, and outputs a succinct sentence describing the token's effect on the predicted probability. I designed the function to categorize the effect strength into three tiers based on absolute weight thresholds: "greatly" (大きく) for weights exceeding 0.10, "moderately" (中程度に) for weights above 0.05, and "slightly" (わずかに) for smaller weights. It further indicates the direction of the effect using "increased" (上げました) or "decreased" (下げました), ensuring a readable and consistent statement across different features and classes.
To provide a high-level overview, I developed summarize_lime_explanation_jp, which constructs a two-sentence narrative for the prediction. This routine first reads the prediction probabilities (predict_proba) to identify the primary and secondary predicted classes. It then maps the local explanation features (local_exp) to tokens via a domain mapper and selects the most significant features, prioritizing those that positively influence the prediction (weights > 0). For the output, the first sentence reports the classification outcome and lists the top three positive contributors to the highest-probability class, along with their formatted weights. The second sentence lists the next three contributing words for the primary class and the top three words supporting the runner-up class, providing a comparative view of the model's decision boundary.

Sample Use Case

Video navigating through the example code: https://youtu.be/ev2dUo76ei4
Code used in the screencast: limejp_tutorial.ipynb

In the provided tutorial, I demonstrate the practical application of the library by guiding users through the classification of Japanese fake news articles. The process begins with the definition of a custom tokenizer using SudachiPy, which I prioritized for its ease of installation, and includes logic to filter punctuation and stopwords to enhance model focus. Utilizing a Random Forest classifier, the tutorial establishes a baseline performance on the "Real" versus "Fake" dataset before deploying the newly added explanatory functions. 
The core of this demonstration involves explicitly calling summarize_lime_explanation_jp to transform raw prediction data into natural language, thereby verifying the library's capacity to automate narrative generation. The resulting output validates the two-sentence template described in the methodology: the first sentence successfully reports the classification outcome—such as a "Real" prediction—alongside the top positive contributors, while the second sentence provides necessary context by listing secondary factors and counter-evidence for the "Fake" class. 
Furthermore, the tutorial output confirms the implementation of the weight categorization logic, as terms are explicitly labeled with qualitative strength indicators like "Strong," "Medium," and "Weak" corresponding to their numerical influence. This successfully bridges the gap between the internal mathematical operations of LIME and user-interpretable text.
Future Directions

As explained in the video, my future directions during the Spring semester is to extend this explanatory function into more personalized, context-faithful way by incorporating LLMs such as ChatGPT or Gemini. The placeholder function that I created for this stage is able to state the model outputs that are already the predetermined facts through calculations. This is able to have smoother connections to the interpretations, but it still needs human eyes to decide the direction of analysis such as whether they would want to use this model or not. By incorporating LLMs, this tool is able to further fill the originally identified gap of bridging model with implications, which LLMs are fairly good at making. 
My current expectation is for LLMs to be able to produce something similar to the final section of the sample use case, where it analyse potential reasons why some words appear more frequently in a certain class over the others based on the given dataset. By feeding in the context using custom prompts, LIME_JP would be able to transcend simple feature attribution, providing a fully automated, context-aware narrative that explains not just which words influenced the prediction, but why they matter within the specific domain.

----

I have already developed some key codes into different repository. However, it is disorganized and unclear on which functionality is located where. It is also forked from the other Github repository, and in order to make my functionality work globally, I need to wait for pull request from the original developer. Since I don't want to wait for pull request from other person and want to deploy on my own schedule, I created a new repository, tokusan, to bring in all of my past work and develop new functionalities on my own repository.

Read over the codes in this public repository if I make a reference to it while developing something1: https://github.com/noyuri2z/lime_jp

Points to keep in mind: 
1. Docstring should be one or two sentences, and inline comments should be as concise as possible. 
2. Avoid writing a code that uses try/except lines.
3. Do not include emoji anywhere in the code or writing. This project is academic project that might be used for writing academic papers, so keep that in mind. 
4. Always ask if the Japanese sentence is correct when generating any kind of Japanese sentences for docstrings/comments or output explanation. 