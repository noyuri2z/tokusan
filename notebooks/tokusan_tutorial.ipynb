{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokusan Tutorial: Japanese-Friendly LIME Explanations\n",
    "\n",
    "This notebook demonstrates how to use the **tokusan** package to explain text classification models with support for Japanese language.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Installation & Setup](#1-installation--setup)\n",
    "2. [Loading the Dataset](#2-loading-the-dataset)\n",
    "3. [Building a Text Classifier](#3-building-a-text-classifier)\n",
    "4. [Creating Explanations with Tokusan](#4-creating-explanations-with-tokusan)\n",
    "5. [Plain Language Explanations (English)](#5-plain-language-explanations-english)\n",
    "6. [Plain Language Explanations (Japanese)](#6-plain-language-explanations-japanese)\n",
    "7. [Visualization](#7-visualization)\n",
    "8. [Advanced Usage](#8-advanced-usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tokusan (run from the project root)\n",
    "# !pip install -e ..\n",
    "\n",
    "# Install Japanese tokenizer support\n",
    "# !pip install sudachipy sudachidict_core\n",
    "\n",
    "# Install additional dependencies for this tutorial\n",
    "# !pip install pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Import tokusan\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from tokusan import (\n",
    "    TextExplainer,\n",
    "    active_japanese_tokenizer,\n",
    "    japanese_splitter,\n",
    "    generate_sentence_for_feature,\n",
    "    generate_sentence_for_feature_jp,\n",
    "    summarize_lime_explanation,\n",
    "    summarize_lime_explanation_jp,\n",
    "    print_lime_narrative,\n",
    "    print_lime_narrative_jp\n",
    ")\n",
    "\n",
    "print(\"Tokusan imported successfully!\")\n",
    "print(f\"Active Japanese tokenizer: {active_japanese_tokenizer()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "\n",
    "We'll use a Japanese fake news dataset for this tutorial. The dataset contains news articles labeled as real (0) or fake (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('fakenews.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['isfake'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to binary classification: real (0) vs fake (2)\n",
    "# We exclude label 1 for cleaner binary classification\n",
    "df_binary = df[df['isfake'].isin([0, 2])].copy()\n",
    "\n",
    "# Convert labels: 0 = Real, 1 = Fake\n",
    "df_binary['label'] = (df_binary['isfake'] == 2).astype(int)\n",
    "\n",
    "print(f\"Binary dataset shape: {df_binary.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df_binary['label'].value_counts())\n",
    "print(\"\\n0 = Real news, 1 = Fake news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a smaller subset for faster training (optional)\n",
    "# Use the full dataset if you have time\n",
    "SAMPLE_SIZE = 2000\n",
    "\n",
    "if len(df_binary) > SAMPLE_SIZE:\n",
    "    df_sample = df_binary.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "else:\n",
    "    df_sample = df_binary\n",
    "\n",
    "print(f\"Sample size: {len(df_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Text Classifier\n",
    "\n",
    "We'll build a simple text classifier using TF-IDF and Logistic Regression. For Japanese text, we need to use a custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom tokenizer for Japanese text\n",
    "def japanese_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenize Japanese text using tokusan's splitter.\n",
    "    Removes punctuation and short tokens.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Get tokens using tokusan's Japanese splitter\n",
    "    tokens = japanese_splitter(text)\n",
    "    \n",
    "    # Filter out punctuation and very short tokens\n",
    "    punctuation_pattern = re.compile(r'^[\\s\\.,!?;:\"\\'\\'\\\"\\(\\)\\[\\]\\{\\}\\-\\—\\–\\.\\。\\、\\！\\？\\「\\」\\『\\』\\（\\）\\・]+$')\n",
    "    \n",
    "    filtered_tokens = [\n",
    "        token for token in tokens \n",
    "        if len(token) > 1 and not punctuation_pattern.match(token)\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"これは日本語のテストです。機械学習モデルを説明します。\"\n",
    "tokens = japanese_tokenizer(test_text)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df_sample['context'].values\n",
    "y = df_sample['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer with Japanese tokenizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=japanese_tokenizer,\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# Build the classifier pipeline\n",
    "classifier = make_pipeline(\n",
    "    vectorizer,\n",
    "    LogisticRegression(max_iter=1000, random_state=42)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nModel Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Explanations with Tokusan\n",
    "\n",
    "Now let's use tokusan to explain individual predictions. The key class is `TextExplainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the explainer for Japanese text\n",
    "explainer = TextExplainer(\n",
    "    class_names=['Real', 'Fake'],  # Class names for display\n",
    "    lang='jp',                      # Enable Japanese tokenization\n",
    "    random_state=42                 # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"TextExplainer created with Japanese language support.\")\n",
    "print(f\"Class names: {explainer.class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample to explain\n",
    "sample_idx = 0\n",
    "sample_text = X_test[sample_idx]\n",
    "true_label = y_test[sample_idx]\n",
    "\n",
    "print(f\"Sample text (first 200 chars): {sample_text[:200]}...\")\n",
    "print(f\"\\nTrue label: {'Fake' if true_label == 1 else 'Real'}\")\n",
    "\n",
    "# Get model prediction\n",
    "pred_proba = classifier.predict_proba([sample_text])[0]\n",
    "print(f\"\\nModel prediction:\")\n",
    "print(f\"  Real: {pred_proba[0]:.3f}\")\n",
    "print(f\"  Fake: {pred_proba[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanation\n",
    "print(\"Generating LIME explanation...\")\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    sample_text,\n",
    "    classifier.predict_proba,\n",
    "    num_features=10,      # Number of top features to include\n",
    "    num_samples=1000,     # Number of perturbations (more = more accurate)\n",
    "    top_labels=2          # Explain top 2 labels\n",
    ")\n",
    "\n",
    "print(\"Explanation generated!\")\n",
    "print(f\"\\nPrediction probabilities: {explanation.predict_proba}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get explanation as a list of (word, weight) tuples\n",
    "print(\"=\" * 60)\n",
    "print(\"Explanation for 'Fake' class (label=1):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fake_explanation = explanation.as_list(label=1)\n",
    "for word, weight in fake_explanation:\n",
    "    direction = \"↑\" if weight > 0 else \"↓\"\n",
    "    print(f\"  {direction} {word}: {weight:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get explanation for 'Real' class\n",
    "print(\"=\" * 60)\n",
    "print(\"Explanation for 'Real' class (label=0):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "real_explanation = explanation.as_list(label=0)\n",
    "for word, weight in real_explanation:\n",
    "    direction = \"↑\" if weight > 0 else \"↓\"\n",
    "    print(f\"  {direction} {word}: {weight:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plain Language Explanations (English)\n",
    "\n",
    "Tokusan can generate natural language explanations that are easier to understand than raw weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate English sentence for individual features\n",
    "print(\"=\" * 60)\n",
    "print(\"Individual Feature Explanations (English):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word, weight in fake_explanation[:5]:\n",
    "    sentence = generate_sentence_for_feature(word, weight, \"Fake\")\n",
    "    print(f\"\\n• {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full English summary\n",
    "print(\"=\" * 60)\n",
    "print(\"Full English Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "english_summary = summarize_lime_explanation(explanation, class_idx=1)\n",
    "for sentence in english_summary:\n",
    "    print(f\"\\n• {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted narrative\n",
    "print_lime_narrative(explanation, class_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plain Language Explanations (Japanese)\n",
    "\n",
    "Tokusan's key feature is generating explanations in natural Japanese language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Japanese sentence for individual features\n",
    "print(\"=\" * 60)\n",
    "print(\"個々の特徴量の説明（日本語）:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word, weight in fake_explanation[:5]:\n",
    "    sentence = generate_sentence_for_feature_jp(word, weight, \"フェイク\")\n",
    "    print(f\"\\n・{sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Japanese summary\n",
    "# Note: The Japanese summary function uses class names from the explanation object\n",
    "print(\"=\" * 60)\n",
    "print(\"日本語による総合説明:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "japanese_summary = summarize_lime_explanation_jp(explanation, class_idx=1)\n",
    "for sentence in japanese_summary:\n",
    "    print(f\"\\n・{sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted Japanese narrative\n",
    "print_lime_narrative_jp(explanation, class_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Tokusan can create visualizations of feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matplotlib bar chart for 'Fake' class\n",
    "fig = explanation.as_pyplot_figure(label=1, figsize=(10, 6))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matplotlib bar chart for 'Real' class\n",
    "fig = explanation.as_pyplot_figure(label=0, figsize=(10, 6))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom visualization comparing both classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, (label, label_name) in enumerate([(0, 'Real'), (1, 'Fake')]):\n",
    "    exp_list = explanation.as_list(label=label)\n",
    "    \n",
    "    words = [w for w, _ in exp_list]\n",
    "    weights = [wt for _, wt in exp_list]\n",
    "    \n",
    "    colors = ['green' if w > 0 else 'red' for w in weights]\n",
    "    \n",
    "    axes[idx].barh(range(len(words)), weights, color=colors)\n",
    "    axes[idx].set_yticks(range(len(words)))\n",
    "    axes[idx].set_yticklabels(words)\n",
    "    axes[idx].set_xlabel('Weight')\n",
    "    axes[idx].set_title(f'Feature Importance for \"{label_name}\"')\n",
    "    axes[idx].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Usage\n",
    "\n",
    "Let's explore some advanced features of tokusan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain multiple samples\n",
    "print(\"Explaining multiple samples...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(3):\n",
    "    text = X_test[i]\n",
    "    true = y_test[i]\n",
    "    pred = classifier.predict([text])[0]\n",
    "    \n",
    "    exp = explainer.explain_instance(\n",
    "        text,\n",
    "        classifier.predict_proba,\n",
    "        num_features=5,\n",
    "        num_samples=500\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Text: {text[:100]}...\")\n",
    "    print(f\"  True: {'Fake' if true == 1 else 'Real'}, Predicted: {'Fake' if pred == 1 else 'Real'}\")\n",
    "    print(f\"  Top words for prediction:\")\n",
    "    \n",
    "    for word, weight in exp.as_list(label=pred)[:3]:\n",
    "        print(f\"    - {word}: {weight:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using plain text explanation method\n",
    "sample_text = X_test[0]\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    sample_text,\n",
    "    classifier.predict_proba,\n",
    "    num_features=5,\n",
    "    num_samples=500\n",
    ")\n",
    "\n",
    "# Get simple plain text summary\n",
    "plain_summary = explainer.explain_instance_plain_text(exp, label=1, n_words=3)\n",
    "print(\"Plain text summary:\")\n",
    "print(plain_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level explanation (useful for some models)\n",
    "char_explainer = TextExplainer(\n",
    "    class_names=['Real', 'Fake'],\n",
    "    char_level=True,  # Enable character-level analysis\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Use a short text for character-level\n",
    "short_text = X_test[0][:50]\n",
    "print(f\"Analyzing text at character level: {short_text}\")\n",
    "\n",
    "char_exp = char_explainer.explain_instance(\n",
    "    short_text,\n",
    "    classifier.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=200\n",
    ")\n",
    "\n",
    "print(\"\\nMost important characters:\")\n",
    "for char, weight in char_exp.as_list(label=1)[:5]:\n",
    "    print(f\"  '{char}': {weight:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save explanation to HTML file\n",
    "exp = explainer.explain_instance(\n",
    "    X_test[0],\n",
    "    classifier.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=500,\n",
    "    top_labels=2\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "exp.save_to_file('explanation_output.html')\n",
    "print(\"Explanation saved to 'explanation_output.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we demonstrated:\n",
    "\n",
    "1. **Setting up tokusan** with Japanese language support\n",
    "2. **Building a text classifier** for Japanese fake news detection\n",
    "3. **Generating LIME explanations** using `TextExplainer`\n",
    "4. **Plain language explanations** in both English and Japanese\n",
    "5. **Visualization** of feature importances\n",
    "6. **Advanced features** like character-level analysis and HTML export\n",
    "\n",
    "### Key Functions Reference:\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `TextExplainer(lang='jp')` | Create explainer with Japanese support |\n",
    "| `explainer.explain_instance()` | Generate explanation for a text |\n",
    "| `explanation.as_list(label)` | Get (word, weight) tuples |\n",
    "| `explanation.as_pyplot_figure()` | Create matplotlib visualization |\n",
    "| `generate_sentence_for_feature()` | English sentence for one feature |\n",
    "| `generate_sentence_for_feature_jp()` | Japanese sentence for one feature |\n",
    "| `summarize_lime_explanation()` | Full English summary |\n",
    "| `summarize_lime_explanation_jp()` | Full Japanese summary |\n",
    "| `print_lime_narrative()` | Print formatted English narrative |\n",
    "| `print_lime_narrative_jp()` | Print formatted Japanese narrative |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
